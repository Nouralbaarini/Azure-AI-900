{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Machine Learning Model with Automated ML\n",
    "\n",
    "\n",
    "In this notebook we'll be using Azure Automated ML to train a machine learning model capable of determining the best cluster for a COVID-19 scientific article. It builds upon the work done in the *Data Preparation* notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import Azure ML SDK modules needed, and do a quick sanity-check on the SDK version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Dataset, Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.automl.core.featurization.featurizationconfig import FeaturizationConfig\n",
    "\n",
    "print(\"AML SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by retrieving the ML workspace used to manage our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve your ML workspace\n",
    "ws = Workspace.from_config()\n",
    "print(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to launch an Automated ML run we need to provision a compute cluster first. If one already exists then we'll use that one, otherwise we'll create a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the compute instance\n",
    "compute_name = 'aml-compute-cpu'\n",
    "# The minimum and maximum number of nodes of the compute instance\n",
    "compute_min_nodes = 0\n",
    "# Setting the number of maximum nodes to a higher value will allow Automated ML to run more experiments in parallel, but will also inccrease your costs\n",
    "compute_max_nodes = 4\n",
    "\n",
    "vm_size = 'STANDARD_DS3_V2'\n",
    "\n",
    "# Check existing compute targets in the workspace for a compute with this name\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(f'Found existing compute target: {compute_name}')    \n",
    "else:\n",
    "    print(f'A new compute target is needed: {compute_name}')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # Create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # Wait for provisioning to complete\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Automated ML experiment\n",
    "\n",
    "We'll use the `COVID19Articles_Train` dataset that we registered in the previous notebook for training the model. In order to speed up training we'll ignore all columns except the word vectors calculated using Doc2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the COVID19Articles_Train dataset from the workspace\n",
    "train_data = Dataset.get_by_name(ws, 'COVID19Articles_Train')\n",
    "\n",
    "# Ignore all columns except the word vectors\n",
    "columns_to_ignore = ['sha', 'source_x', 'title', 'doi', 'pmcid', 'pubmed_id', 'license', 'abstract', 'publish_time', 'authors', 'journal', 'mag_id',\n",
    "                     'who_covidence_id', 'arxiv_id', 'pdf_json_files', 'pmc_json_files', 'url', 's2_id' ]\n",
    "train_data = train_data.drop_columns(columns_to_ignore) \n",
    "\n",
    "\n",
    "# Configura Automated ML\n",
    "automl_config = AutoMLConfig(task = \"classification\",\n",
    "                             # Use weighted area under curve metric to evaluate the models\n",
    "                             primary_metric='AUC_weighted',\n",
    "                             \n",
    "                             # Use all columns except the ones we decided to ignore\n",
    "                             training_data = train_data,\n",
    "                             \n",
    "                             # The values we're trying to predict are in the `cluster` column\n",
    "                             label_column_name = 'cluster',\n",
    "                             \n",
    "                             # Evaluate the model with 5-fold cross validation\n",
    "                             n_cross_validations=5,\n",
    "                             \n",
    "                             # The experiment should be stopped after 15 minutes, to minimize cost\n",
    "                             experiment_timeout_hours=.25,\n",
    "                             \n",
    "                             # Automated ML can try at most 4 models at the same time, this is also limited by the compute instance's maximum number of nodes\n",
    "                             max_concurrent_iterations=4,\n",
    "                             \n",
    "                             # An iteration should be stopped if it takes more than 5 minutes\n",
    "                             iteration_timeout_minutes=5,\n",
    "                             \n",
    "                             compute_target=compute_target\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have configured the Automated ML run, we can submit it in one of the workspace's experiments. Note that this step should take around 15 minutes, according to the `experiment_timeout_minutes` setting.\n",
    "\n",
    "**NOTE**:\n",
    "\n",
    "If this is the first time you are launching an experiment run in the Azure Machine Learning workspace, additional time will be needed to start the Compute Cluster and deploy the container images required to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `COVID19_Classification` dataset\n",
    "exp = Experiment(ws, 'COVID19_Classification')\n",
    "run = exp.submit(automl_config, show_output=True)\n",
    "\n",
    "# Retrieve the best performing run and its corresponding model from the aggregated Automated ML run\n",
    "best_run, best_model = run.get_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the Automated ML run has finished, we can visualize its models and see how they measure up according to several metrics. Remember, the higher the *AUC_weighter*, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
