{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Automated ML model\n",
    "\n",
    "In this notebook you will be loading the best machine learning model trained using Automated ML, and use it to assign clusters to a series of new COVID-19 articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the latest model trained with Automated ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off by importing the necessary modules and checking the Azure ML SDK version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from azureml.core import Workspace, Experiment, Dataset, VERSION\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.interpret import ExplanationClient\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "from interpret_community.widget import ExplanationDashboard\n",
    "\n",
    "print(\"Azure ML SDK Version: \", VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to load our workspace, and use that to retrieve our Automated ML experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the workspace from a configuration file\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get a reference to our automated ml experiment\n",
    "exp = Experiment(ws, 'COVID19_Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to retrieve our latest Automated ML run, and its corresponding best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a list of all the experiment's runs\n",
    "runs = list(exp.get_runs()) \n",
    "\n",
    "# Pick the latest run\n",
    "raw_run = runs[len(runs)-1]\n",
    "\n",
    "# Convert it to an AutoMLRun object in order to retrieve its best model\n",
    "automl_run = AutoMLRun(exp, raw_run.id)\n",
    "\n",
    "# Retrieve the best run and its corresponding model\n",
    "best_run, best_model = automl_run.get_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the metrics calculated while training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having retrieved the best performing run, let's examine some of its metrics using the SDK's `RunDetails` widget. Analyze the various metrics of your model, including *Precision-Recall*, *ROC*, *Lift Curve*, *Gain Curve*, and *Calibration Curve*.\n",
    "\n",
    "Analyze the *Confusion Matrix* and see which clusters are correctly identified by the model, and which have a higher likelihood ofbeing misclassified.\n",
    "\n",
    "Experiment with the *Feature Importance* and analyze the relative importance of the top K features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(best_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model on a new dataset\n",
    "\n",
    "First we'll load the dataset we had previously prepared for testing, and convert it to a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the dataset from the workspace\n",
    "test_ds = Dataset.get_by_name(ws, 'COVID19Articles_Test')\n",
    "\n",
    "# Convert it to a standard pandas data frame\n",
    "test_df = test_ds.to_pandas_dataframe()\n",
    "\n",
    "# Examine a sample of 5 documents\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the *best_model* to assign clusters to the test documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the true values of the clusters\n",
    "true_clusters = test_df['cluster']\n",
    "\n",
    "# Keep all features except the label column\n",
    "features_df = test_df.drop(columns=['cluster'])\n",
    "\n",
    "# Predict the clusters for each document and display them\n",
    "best_model.predict(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the true clusters with the predicted ones by using a confusion matrix - notice the true positive values on the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(best_model, features_df, true_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting and explaining the model\n",
    "\n",
    "By default, Automated ML also explains the machine learning models it trains. We will download and examine the explanations for our *best_model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an ExplanationClient for accesing the best run's model explanations\n",
    "client = ExplanationClient.from_run(best_run)\n",
    "\n",
    "# Download the engineered explanations in their raw form\n",
    "engineered_explanations = client.download_model_explanation(raw=True)\n",
    "\n",
    "# Retrieve the dataset used for training the model - it will be needed when visualizing the explanations\n",
    "training_df = Dataset.get_by_name(ws, 'COVID19Articles_Train').to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an `ExplanationDashboard` to visualize the engineered explanations. For best results it needs to be presented with the same dataset used for training the model.\n",
    "\n",
    "Analyze the *Aggregate Feature Importance* to identify the top predictive features. Select a feature and analyze how individual values of that feature impact prediction results. Switch to the *Individual Feature Importance & What-If* and explore the feature importance plots for individual points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(engineered_explanations, best_model, dataset=training_df.drop(columns='cluster'), true_y=training_df['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}