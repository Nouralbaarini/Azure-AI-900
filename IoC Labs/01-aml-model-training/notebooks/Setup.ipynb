{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the COVID-19 Open Research Dataset\n",
    "\n",
    "The COVID-19 Open Research Dataset (CORD-19) is a collection of over 50,000 scholarly articles - including over 40,000 with full text - about COVID-19, SARS-CoV-2, and related coronaviruses. This dataset has been made freely available with the goal to aid research communities combat the COVID-19 pandemic. It has been made available by the Allen Institute for AI in partnership with leading research groups to prepare and distribute the COVID-19 Open Research Dataset (CORD-19), in response to the COVID-19 pandemic.\n",
    "\n",
    "During this lab you will learn how to process and analyze a subset of the articles present in the dataset, group them together into a series of clusters, and use Automated ML to train a machine learning model capable of classifying new articles as they are published."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We will start off by installing a few packages, such as `nltk` for text processing and `wordcloud`, `seaborn`, and `yellowbrick` for various visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install wordcloud\n",
    "!pip install seaborn\n",
    "!pip install yellowbrick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first download stopwords and the Punkt tokenizer models present in the `nltk` package, in order to be able to process the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also import the rest of the modules needed in this notebook, and do a quick sanity-check on the Azure ML SDK version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1610442370993
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_palette('Set2')\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans, SpectralClustering, DBSCAN, Birch, AgglomerativeClustering\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "\n",
    "from azureml.core import Workspace, Datastore, Dataset, VERSION\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Dataset, Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.automl.core.featurization.featurizationconfig import FeaturizationConfig\n",
    "\n",
    "print(\"Azure ML SDK Version: \", VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Covid-19 data\n",
    "\n",
    "CORD-19 has been uploaded as an Azure Open Dataset, we will connect to it and use it's API to download the dataset locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1610442384900
    }
   },
   "outputs": [],
   "source": [
    "covid_dirname = 'covid19temp'\n",
    "\n",
    "cord19_dataset = Dataset.File.from_files('https://azureopendatastorage.blob.core.windows.net/' + covid_dirname)\n",
    "mount = cord19_dataset.mount()\n",
    "\n",
    "covid_dirpath = os.path.join(mount.mount_point, covid_dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a sample of the dataset (top 5 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1610442521618
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "mount.start()\n",
    "\n",
    "# Load metadata.csv, as it contains a list of all the articles and their corresponding titles/authors/contents.\n",
    "metadata_filename = os.path.join(covid_dirpath, 'metadata.csv')\n",
    "\n",
    "metadata = pd.read_csv(metadata_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the articles do not have any associated documents, so we will filter those out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "metadata_with_docs = metadata[metadata['pdf_json_files'].isna() == False]\n",
    "\n",
    "print(f'Dataset contains {metadata.shape[0]} entries, out of which {metadata_with_docs.shape[0]} have associated json documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the percentage of items in the dataset that have associated JSON documents (research papers that have extra metadata associated with them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Change the document index in order to preview a different article\n",
    "DOCUMENT_INDEX = 0 \n",
    "example_entry = metadata_with_docs.iloc[DOCUMENT_INDEX]\n",
    "\n",
    "filepath = os.path.join(covid_dirpath, example_entry['pdf_json_files'])\n",
    "print(f'Document local filepath: {filepath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will display the list of elements that are available for the selected document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "except FileNotFoundError as e:\n",
    "    # in case the mount context has been closed\n",
    "    mount.start()\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "print(f'Data elements: { \", \".join(data.keys())}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the full text version of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = nltk.corpus.stopwords.words('english') + list(punctuation) + ['et', 'al.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader:\n",
    "    \"\"\"Class used to read the files associated with an article\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    def read_file_to_json(self, filepath):\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "        except FileNotFoundError as e:\n",
    "            mount.start()\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "        return data\n",
    "    \n",
    "    def parse_document(self, document_index):\n",
    "        document = metadata_with_docs.iloc[document_index]\n",
    "        \n",
    "        # One article can have multiple associated documents\n",
    "        words = []\n",
    "        for filename in document['pdf_json_files'].split('; '):\n",
    "            filepath = '{0}/{1}'.format(covid_dirpath, filename)\n",
    "            data = self.read_file_to_json(filepath)\n",
    "\n",
    "            # Split each paragraph into multiple sentences first, in order to improve the performance of the word tokenizer\n",
    "            text = data['body_text']\n",
    "            for paragraph in text:\n",
    "                p_sentences = sent_tokenize(paragraph['text'])\n",
    "\n",
    "                # Split each sentence into words, while making sure to remove the stopwords and stem the words\n",
    "                for p_sentence in p_sentences:\n",
    "                    sentence = [ self.stemmer.stem(word) for word in word_tokenize(p_sentence) if word.isalpha() and word.lower() not in stop_tokens ]\n",
    "                    words.extend(sentence)\n",
    "    \n",
    "        return (words, document['cord_uid'])\n",
    "        \n",
    "\n",
    "class Corpus:\n",
    "    \"\"\"An iterator that reads all sentences from the first N documents\"\"\"\n",
    "    \n",
    "    def __init__(self, n_documents):\n",
    "        self.n_documents = n_documents\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.reader = Reader()\n",
    "        \n",
    "    def __iter__(self):\n",
    "         for document_index in range(0, self.n_documents):   \n",
    "            words, document_id = self.reader.parse_document(document_index)\n",
    "            yield TaggedDocument(words, document_id)\n",
    "            \n",
    "    def plain_iter(self):\n",
    "        for document_index in range(0, self.n_documents):  \n",
    "            words, document_id = self.reader.parse_document(document_index)\n",
    "            yield (words, document_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding documents as vectors\n",
    "\n",
    "In this lab, we're using a subset of 500 articles to train a Machine Learning model that encodes text documents into numerical vectors (a document embedding model). \n",
    "\n",
    "Training a document embedding model takes a significant amount of time, and for this reason we already provide a trained model. We also provide the code below in case you want to get more details about the process. Running the next two cells will result in loading the already existing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DOCUMENTS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_filename = f'covid_embeddings_model_{N_DOCUMENTS}_docs.w2v'\n",
    "\n",
    "if (os.path.exists(model_filename)):\n",
    "    model = Doc2Vec.load(model_filename)\n",
    "    print(f'Done, loaded word2vec model with { len(model.wv.vocab) } words.')\n",
    "else:\n",
    "    model = Doc2Vec(Corpus(N_DOCUMENTS), vector_size=128, batch_words=10)\n",
    "    model.save(model_filename)\n",
    "    print(f'Done, trained word2vec model with { len(model.wv.vocab) } words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequencies\n",
    "\n",
    "Let's analyze the relative frequencies of words in the corpus of articles. We will display a word cloud to provide a visual representation of these relative frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "word_vectors = []\n",
    "ids = []\n",
    "\n",
    "for (words, doc_id) in Corpus(N_DOCUMENTS).plain_iter():\n",
    "    ids.append(doc_id)\n",
    "    word_vector = model.infer_vector(words)\n",
    "    word_vectors.append(word_vector)\n",
    "    if len(word_vectors) % 100 == 0:\n",
    "        print(f'Processed {len(word_vectors)} documents.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've finished reading the articles, we can dismount the dataset in order to free up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mount.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_df = pd.DataFrame(word_vectors, index=ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll join the DataFrame containing the numerical embeddings with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_metadata = metadata_with_docs.set_index('cord_uid')\n",
    "metadata_with_embeddings = pd.concat([indexed_metadata.iloc[:N_DOCUMENTS], wv_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering documents\n",
    "\n",
    "We've determined the acceptable value for the clusters, so let's use Machine Learning to determine those clusters. We'll use the classic KMeans algorithm to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = KElbowVisualizer(KMeans(), k=(3,20))\n",
    "visualizer.fit(wv_df)\n",
    "clusterer = KMeans(12 if visualizer.elbow_value_ > 12 else visualizer.elbow_value_)\n",
    "clusterer.fit(wv_df)\n",
    "clusters = clusterer.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add each article's cluster as new column to our combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_with_clusters = metadata_with_embeddings\n",
    "metadata_with_clusters['cluster'] = clusters\n",
    "metadata_with_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now split our data into two datasets - a **training** one that will be used to train a Machine Learning model, able to determine the cluster that should be assigned to an article, and a **test** one that we'll use to test this classifier.\n",
    "\n",
    "We will allocate 80% of the articles to training the Machine Learning model, and the remaining 20% to testing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(metadata_with_clusters, train_size=0.8)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up training, we'll ignore all columns except the word vectors calculated using Doc2Vec. For this reason, we will create a separate dataset just with the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_ignore = ['sha', 'source_x', 'title', 'doi', 'pmcid', 'pubmed_id', 'license', 'abstract', 'publish_time', 'authors', 'journal', 'mag_id',\n",
    "                     'who_covidence_id', 'arxiv_id', 'pdf_json_files', 'pmc_json_files', 'url', 's2_id' ]\n",
    "train_data_vectors = train.drop(columns_to_ignore, axis=1)\n",
    "test_data_vectors = test.drop(columns_to_ignore, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the training and testing datasets for AutoML availability\n",
    "\n",
    "We're registering the training and testing datasets with the Azure Machine Learning datastore to make them available inside Azure Machine Learning Studio and Automated ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve your ML workspace\n",
    "ws = Workspace.from_config()\n",
    "# Retrieve the workspace's default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "Dataset.Tabular.register_pandas_dataframe(train_data_vectors, datastore, 'COVID19Articles_Train')\n",
    "Dataset.Tabular.register_pandas_dataframe(test_data_vectors, datastore, 'COVID19Articles_Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the compute instance\n",
    "compute_name = 'aml-compute-cpu'\n",
    "# The minimum and maximum number of nodes of the compute instance\n",
    "compute_min_nodes = 1\n",
    "# Setting the number of maximum nodes to a higher value will allow Automated ML to run more experiments in parallel, but will also inccrease your costs\n",
    "compute_max_nodes = 1\n",
    "\n",
    "vm_size = 'STANDARD_DS3_V2'\n",
    "\n",
    "# Check existing compute targets in the workspace for a compute with this name\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(f'Found existing compute target: {compute_name}')    \n",
    "else:\n",
    "    print(f'A new compute target is needed: {compute_name}')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # Create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # Wait for provisioning to complete\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the COVID19Articles_Train dataset from the workspace\n",
    "train_data = Dataset.get_by_name(ws, 'COVID19Articles_Train')\n",
    "\n",
    "\n",
    "\n",
    "# Configura Automated ML\n",
    "automl_config = AutoMLConfig(task = \"classification\",\n",
    "                             # Use weighted area under curve metric to evaluate the models\n",
    "                             primary_metric='AUC_weighted',\n",
    "                             \n",
    "                             # Use all columns except the ones we decided to ignore\n",
    "                             training_data = train_data,\n",
    "                             \n",
    "                             # The values we're trying to predict are in the `cluster` column\n",
    "                             label_column_name = 'cluster',\n",
    "                             \n",
    "                             # Evaluate the model with 5-fold cross validation\n",
    "                             n_cross_validations=5,\n",
    "                             \n",
    "                             # The experiment should be stopped after 15 minutes, to minimize cost\n",
    "                             experiment_timeout_hours=.25,\n",
    "                             #blocked_models=['XGBoostClassifier'],\n",
    "                             \n",
    "                             # Automated ML can try at most 1 models at the same time, this is also limited by the compute instance's maximum number of nodes\n",
    "                             max_concurrent_iterations=1,\n",
    "                             \n",
    "                             # An iteration should be stopped if it takes more than 5 minutes\n",
    "                             iteration_timeout_minutes=3,\n",
    "                             \n",
    "                             compute_target=compute_target,\n",
    "                             \n",
    "                             #The total number of different algorithm and parameter combinations to test during an automated ML experiment. If not specified, the default is 1000 iterations.\n",
    "                             iterations = 5\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `COVID19Articles_Train_Vectors` dataset\n",
    "exp = Experiment(ws, 'COVID19_Classification')\n",
    "run = exp.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve the best performing run and its corresponding model from the aggregated Automated ML run\n",
    "best_run, best_model = run.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}